# -*- coding: utf-8 -*-
"""zomato_restaurant_clustering _project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JXDnGFG6VusYKE6ajchlSJZlQvu_S4HG

# **Project Name**    -  **Zomato Restaurant Clustering**

##### **Project Type**    - EDA + NLP + Unsupervised Learning (Clustering)

##### **Contribution**    - Individual

# **Project Summary -**

- Analyzed **Zomato restaurant data** to understand customer behavior and restaurant performance across Indian cities
- Performed **data cleaning, preprocessing, and sentiment analysis** on customer reviews
- Engineered meaningful features using **cost, cuisine diversity, review behavior, and sentiment scores**
- Applied **unsupervised learning techniques** to cluster restaurants into actionable segments
- Compared **K-Means, Hierarchical, and DBSCAN** clustering models using internal evaluation metrics
- **K-Means** selected as the final model due to better cluster separation and business interpretability
- Generated insights that help **customers choose better restaurants** and enable **Zomato and restaurant** **owners** to improve service quality, pricing, and targeting
- Final model was saved and validated for **deployment readiness**

# **GitHub Link -**

**github.com/shivakumar242**

# **Problem Statement**

**Key Questions Addressed**

- How can **negative customer sentiment** be identified and quantified effectively?
- How do review **volume and review length** influence restaurant perception?
- Can restaurants with **similar pricing but different sentiment** be distinguished?
- How can clustering help identify **underperforming restaurants**?
- How can insights be made **actionable for business stakeholders**?
- How can restaurants be compared **across different cities**?

**Difficult Situations Faced**

- Reviews contain **noise, slang, and mixed emotions**
- Sentiment scores may **not always align with ratings**
- Cost data shows **extreme variations across cities**
- Feature scaling was required to avoid **distance bias in clustering**
- Choosing features without causing **overfitting or redundancy**
- Interpreting clusters in **business terms**, not just technical labels
- Some clusters showed **overlap**, making interpretation difficult
- Ensuring results are **explainable and stakeholder-friendly**

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

# Import Libraries

# Suppress warnings for clean output
import warnings
warnings.filterwarnings("ignore")

# Core libraries
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# NLP
import re
import nltk
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer

# ML
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors

# Utilities
import joblib

# NLTK RESOURCES
nltk_packages = [
    'stopwords',
    'vader_lexicon'
]

for pkg in nltk_packages:
    try:
        nltk.data.find(f'corpora/{pkg}')
    except LookupError:
        nltk.download(pkg)

print("Global setup completed successfully")

"""### Dataset Loading"""

# Load Dataset

from google.colab import drive
drive.mount('/content/drive')

META_PATH = "/content/drive/MyDrive/project/Zomato_Restaurant_Clustering/Zomato Restaurant names and Metadata.csv"
REVIEWS_PATH = "/content/drive/MyDrive/project/Zomato_Restaurant_Clustering/Zomato Restaurant reviews.csv"

meta_df = pd.read_csv(META_PATH)
reviews_df = pd.read_csv(REVIEWS_PATH)

print("Datasets loaded successfully")

"""### Dataset First View"""

# Dataset First Look

display(meta_df.head())
display(reviews_df.head())

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count

print("Metadata Shape :", meta_df.shape)
print("Reviews Shape  :", reviews_df.shape)

"""### Dataset Information"""

# Dataset Info

meta_df.info()
print("-" * 50)
reviews_df.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count

# Check duplicate rows in metadata
meta_duplicates = meta_df.duplicated().sum()
print(f"Duplicate rows in Restaurant Metadata dataset: {meta_duplicates}")

# Check duplicate rows in reviews
reviews_duplicates = reviews_df.duplicated().sum()
print(f"Duplicate rows in Restaurant Reviews dataset: {reviews_duplicates}")

# Remove duplicates only from reviews (safe & logical)
reviews_df = reviews_df.drop_duplicates()

# Verify after removal
print("Duplicates after removal (Reviews):", reviews_df.duplicated().sum())

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count

# Missing values count in metadata
print("Missing values in Restaurant Metadata dataset:")
display(meta_df.isnull().sum())

print("-" * 50)

# Missing values count in reviews
print("Missing values in Restaurant Reviews dataset:")
display(reviews_df.isnull().sum())

# Visualizing the missing values

plt.figure(figsize=(10, 4))
sns.heatmap(meta_df.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values Heatmap – Restaurant Metadata Dataset")
plt.show()

plt.figure(figsize=(10, 4))
sns.heatmap(reviews_df.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values Heatmap – Restaurant Reviews Dataset")
plt.show()

"""### What did you know about your dataset?

- The dataset includes restaurant metadata and customer reviews from Zomato.

- It combines numerical, categorical, and textual data, making it suitable for EDA, sentiment analysis, and clustering.

- Customer reviews provide deeper insights into satisfaction beyond ratings.

## ***2. Understanding Your Variables***
"""

# Dataset Columns

# Display column names of Restaurant Metadata dataset
print("Restaurant Metadata Columns:")
display(meta_df.columns)

print("\nRestaurant Reviews Columns:")
display(reviews_df.columns)

# Dataset Describe

# Statistical summary of numerical columns
print("Restaurant Metadata - Numerical Summary")
display(meta_df.describe())

print("\nRestaurant Reviews - Numerical Summary")
display(reviews_df.describe())

"""### Variables Description

- Numerical variables are mainly used for **statistical analysis and clustering**.

- Textual variables are used for **sentiment analysis**.

- Categorical variables help in **segmentation and grouping**.

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.

# Unique values count for Restaurant Metadata dataset
print("Unique values count – Restaurant Metadata")
for col in meta_df.columns:
    print(f"{col}: {meta_df[col].nunique()}")

print("\n" + "-"*50 + "\n")

# Unique values count for Restaurant Reviews dataset
print("Unique values count – Restaurant Reviews")
for col in reviews_df.columns:
    print(f"{col}: {reviews_df[col].nunique()}")

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

# Write your code to make your dataset analysis ready.

# DATA WRANGLING

# 1. Handle Missing Values

# Metadata dataset
meta_df['Collections'] = meta_df['Collections'].fillna('Not Specified')
meta_df['Timings'] = meta_df['Timings'].fillna('Not Available')
meta_df['Cost'] = pd.to_numeric(meta_df['Cost'], errors='coerce')
meta_df['Cost'].fillna(meta_df['Cost'].median(), inplace=True)
meta_df['Cuisines'] = meta_df['Cuisines'].fillna('Unknown')

# Reviews dataset
reviews_df['Reviewer'] = reviews_df['Reviewer'].fillna('Anonymous')
reviews_df['Metadata'] = reviews_df['Metadata'].fillna('Unknown')
reviews_df['Time'] = reviews_df['Time'].fillna('Unknown')

reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')
reviews_df.dropna(subset=['Rating'], inplace=True)
reviews_df['Review'] = reviews_df['Review'].fillna("")

# 2. Basic Text Cleaning (Analysis Ready)
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

reviews_df['clean_review'] = reviews_df['Review'].apply(clean_text)

# 3. Feature Engineering
reviews_df['review_length'] = reviews_df['clean_review'].apply(len)
meta_df['cuisine_count'] = meta_df['Cuisines'].apply(
    lambda x: len(str(x).split(','))
)

# 4. Final Sanity Check
print("Final Metadata Shape:", meta_df.shape)
print("Final Reviews Shape:", reviews_df.shape)

print("\nRemaining Missing Values (Metadata):")
print(meta_df.isnull().sum())

print("\nRemaining Missing Values (Reviews):")
print(reviews_df.isnull().sum())

"""### What all manipulations have you done and insights you found?

**What was done**

- Removed duplicate reviews to avoid bias.
- Handled missing values using median imputation and logical defaults.
- Standardized data types for cost and ratings.
- Cleaned review text for analysis readiness.
- Engineered key features: **review length** and **cuisine count**.

**Key insights**

- Review engagement varies widely across restaurants.
- Cuisine diversity differs significantly and can influence customer choice.
- Cleaned data is consistent and ready for reliable analysis and modeling.

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

#### Chart - 1
"""

# Chart 1: Distribution of Restaurant Ratings

plt.figure(figsize=(6, 4))
sns.histplot(reviews_df['Rating'], bins=20, kde=True)
plt.title("Distribution of Restaurant Ratings")
plt.xlabel("Rating")
plt.ylabel("Number of Reviews")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **histogram** is effective for analyzing the **distribution of numerical data** such as customer ratings.

##### 2. What is/are the insight(s) found from the chart?

- Most restaurant ratings are clustered between **3.5 and 4.5**, indicating generally positive customer experiences.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive:**
- Maintaining service quality within this rating range helps restaurants retain customers and improve platform visibility.

**Negative:**
- Restaurants with consistently below-average ratings risk reduced customer trust and lower demand.

#### Chart - 2
"""

# Chart 2: Distribution of Restaurant Cost

plt.figure(figsize=(6, 4))
sns.boxplot(y=meta_df['Cost'])
plt.title("Distribution of Restaurant Cost")
plt.ylabel("Average Cost for Two")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **box plot** clearly represents the s**pread, median, and outliers** in restaurant pricing.

##### 2. What is/are the insight(s) found from the chart?

- Most restaurants fall within a **low to mid-cost range**, indicating affordability.
- A small number of **high-cost restaurants appear as outliers**, representing premium offerings.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive:**
- Affordable pricing helps attract a **larger customer base** and drives higher order volume.

**Negative:**
- High-cost restaurants cater to a **limited audience** and may experience lower demand if value perception is weak.

#### Chart - 3
"""

# Chart - 3 visualization code(scatter plot)

plt.figure(figsize=(6,4))
sns.scatterplot(
    x=meta_df['Cost'],
    y=meta_df['cuisine_count']
)
plt.title("Cost vs Cuisine Count")
plt.xlabel("Average Cost for Two")
plt.ylabel("Number of Cuisines")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **scatter plot** is suitable for analyzing the **relationship between restaurant cost and cuisine variety**.

##### 2. What is/are the insight(s) found from the chart?

- Restaurants with **higher costs** generally offer a **greater number of cuisines**.
- Low-cost restaurants mostly provide **limited cuisine options**.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive:**
- Premium pricing enables restaurants to **expand menu variety**, enhancing overall customer value.

**Negative:**
- Budget restaurants may face limitations in offering diverse cuisines, which can **reduce customer appeal**.

#### Chart - 4
"""

# Chart - 4 visualization code(bar chart)

# Cost Category Distribution (All-in-One Safe Cell)

# Step 1: Define fixed cost bins (safe, monotonic)
cost_bins = [0, 300, 600, 1000, 2000]
cost_labels = ['Low Cost', 'Medium Cost', 'High Cost', 'Premium']

# Step 2: Create cost category safely
meta_df['cost_category'] = pd.cut(
    meta_df['Cost'],
    bins=cost_bins,
    labels=cost_labels
)

# Step 3: Plot
plt.figure(figsize=(6,4))
meta_df['cost_category'].value_counts().plot(kind='bar')
plt.title("Distribution of Restaurants by Cost Category")
plt.xlabel("Cost Category")
plt.ylabel("Number of Restaurants")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **bar chart** is suitable for comparing the **distribution of restaurants across different price categories**.

##### 2. What is/are the insight(s) found from the chart?

- The **Medium Cost** segment dominates the market.
- **Low Cost** restaurants form a smaller share.
- **Premium** restaurants are almost absent.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive:**
- A strong mid-range pricing presence helps attract a **broad customer base** and ensures stable demand.

**Negative:**
- Limited premium offerings may reduce appeal to **high-spending customers**, restricting high-margin opportunities.

#### Chart - 5
"""

# Chart - 5 visualization code

# Review Length Distribution (Violin Plot)

plt.figure(figsize=(6,4))
sns.violinplot(y=reviews_df['review_length'])
plt.title("Distribution of Review Length")
plt.ylabel("Review Length")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **violin plot** clearly shows the **distribution, density, and spread** of review lengths.

##### 2. What is/are the insight(s) found from the chart?

- Most customer reviews are **short**, indicating quick feedback.
- A few **very long reviews** appear as extreme values, representing intense experiences.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive**:
- Short reviews enable faster feedback collection and quicker issue identification.

**Negative:**
- Very long reviews may indicate **serious dissatisfaction**, requiring immediate attention.

#### Chart - 6
"""

# Chart - 6 visualization code

# Share of Restaurants by Cost Category (Pie Chart)

# Ensure cost_category exists
if 'cost_category' not in meta_df.columns:
    cost_bins = [0, 300, 600, 1000, 2000]
    cost_labels = ['Low Cost', 'Medium Cost', 'High Cost', 'Premium']
    meta_df['cost_category'] = pd.cut(meta_df['Cost'], bins=cost_bins, labels=cost_labels)

# Prepare data
cost_share = meta_df['cost_category'].value_counts()

# Plot pie chart
plt.figure(figsize=(6,6))
plt.pie(cost_share, labels=cost_share.index, autopct='%1.1f%%', startangle=140)
plt.title("Share of Restaurants by Cost Category")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **pie chart** is suitable for showing the **proportion and percentage share** of restaurants across cost categories.

##### 2. What is/are the insight(s) found from the chart?

- The **Medium Cost** segment represents the **largest share** of restaurants.
- **Low Cost** and **High Cost** restaurants form smaller portions.
- **Premium** restaurants account for a **very small share**.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business impact**

**Positive:**
- Dominance of mid-range pricing reflects **mass-market demand**, helping platforms attract a wider audience.

**Negative:**
- Limited presence of premium restaurants reduces options for **high-spending customers**, impacting high-margin opportunities.

#### Chart - 7
"""

# Chart 7: Word Cloud of Customer Reviews

from wordcloud import WordCloud

# Combine all cleaned reviews
text = " ".join(reviews_df['clean_review'])

# Generate word cloud
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    max_words=100
).generate(text)

# Plot
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of Customer Reviews")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **word cloud** helps quickly identify the **most frequently used words in customer reviews**, giving an intuitive overview of key discussion themes.

##### 2. What is/are the insight(s) found from the chart?

- Words such as **“food,” “service,” “good,” “taste,” “place,” and “ordered” dominate** the reviews.
- The prominence of positive terms indicates that customers frequently discuss **food quality, service experience, and overall satisfaction**.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business impact**

**Positive:**
- Identifies the **primary drivers of customer satisfaction**, enabling restaurants to focus on food quality and service improvements.

**Negative:**
- If negative words become prominent in future reviews, they may signal **recurring service or quality issues** that require immediate action.

#### Chart - 8
"""

# Chart 8: Rating Variability Across Top Restaurants

# Select top 10 restaurants with the most reviews
top_restaurants = reviews_df['Restaurant'].value_counts().head(10).index
filtered_df = reviews_df[reviews_df['Restaurant'].isin(top_restaurants)]

plt.figure(figsize=(8, 4))
sns.boxplot(x='Restaurant', y='Rating', data=filtered_df)
plt.xticks(rotation=45, ha='right')
plt.title("Rating Variability Across Top Restaurants")
plt.xlabel("Restaurant")
plt.ylabel("Rating")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **box plot** effectively highlights **rating consistency, spread, and outliers** across different restaurants.

##### 2. What is/are the insight(s) found from the chart?

- Some restaurants maintain **consistently high ratings**, indicating reliable service quality.
- Other restaurants show **wide rating variability**, suggesting inconsistent customer experiences.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business impact**

**Positive:**
- Consistent ratings help build **customer trust and long-term loyalty**.

**Negative:**
- High rating variability points to **service quality issues**, which can harm brand reputation if not addressed.

#### Chart - 9
"""

# Chart 9: Review Length vs Rating

plt.figure(figsize=(6, 4))
sns.scatterplot(
    x=reviews_df['review_length'],
    y=reviews_df['Rating'],
    alpha=0.5
)
plt.title("Relationship Between Review Length and Rating")
plt.xlabel("Review Length")
plt.ylabel("Rating")
plt.show()

"""##### 1. Why did you pick the specific chart?

A **scatter plot** helps analyze the **relationship between two numerical variables**, review length and rating.

##### 2. What is/are the insight(s) found from the chart?

- Extremely long reviews tend to appear more often with **very low or very high ratings.**
- Short reviews are spread across **moderate ratings**, indicating neutral feedback.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business impact**

**Positive:**
- Long reviews can highlight **strong customer emotions**, providing valuable feedback for improvement.

**Negative:**
- Long negative reviews can signal **serious dissatisfaction**, posing a reputation risk if ignored.

#### Chart - 10
"""

# Chart 10: Average Rating by Cost Category

avg_rating_by_cost = (
    meta_df.merge(
        reviews_df[['Restaurant', 'Rating']],
        left_on='Name',
        right_on='Restaurant',
        how='inner'
    )
    .groupby('cost_category')['Rating']
    .mean()
)

plt.figure(figsize=(6, 4))
avg_rating_by_cost.plot(kind='bar')
plt.title("Average Rating by Cost Category")
plt.xlabel("Cost Category")
plt.ylabel("Average Rating")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **bar chart** is suitable for comparing **average ratings across different cost categories.**

##### 2. What is/are the insight(s) found from the chart?

- **Medium Cost** restaurants have the **highest average ratings**.
- **High Cost** restaurants follow closely but do not significantly outperform medium-cost options.
- **Low Cost** restaurants show slightly **lower average ratings.**
- The **Premium** category is missing or negligible, indicating insufficient data.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business impact**

**Positive:**
- Medium-priced restaurants strike the **best balance between cost and quality,** leading to higher customer satisfaction.

**Negative:**
- Simply increasing prices does **not guarantee higher ratings,** which may impact premium positioning strategies.

#### Chart - 11
"""

# Chart 11: Average Rating by Cuisine Count

# Merge cuisine count with reviews
rating_cuisine_df = meta_df[['Name', 'cuisine_count']].merge(
    reviews_df[['Restaurant', 'Rating']],
    left_on='Name',
    right_on='Restaurant',
    how='inner'
)

# Group by cuisine count
avg_rating_cuisine = (
    rating_cuisine_df
    .groupby('cuisine_count')['Rating']
    .mean()
    .sort_index()
)

plt.figure(figsize=(6, 4))
avg_rating_cuisine.plot(kind='line', marker='o')
plt.title("Average Rating by Cuisine Count")
plt.xlabel("Number of Cuisines")
plt.ylabel("Average Rating")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **line chart** is suitable for understanding how a**verage ratings change with increasing cuisine variety,** which is an ordered numerical variable.

##### 2. What is/are the insight(s) found from the chart?

- Restaurants offering **3-4 cuisines** tend to receive **higher average ratings.**
- A drop in average rating is observed around **5 cuisines**, indicating possible operational complexity.
- Restaurants with very high **cuisine variety (6 cuisines)** show the **highest average rating,** though this may be influenced by a **small sample size.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business impact**

**Positive:**
- Offering a **moderate to high cuisine variety** can improve customer satisfaction by catering to diverse preferences.

**Negative:**
- Expanding menu variety without operational control may reduce quality consistency and impact ratings.

#### Chart - 12
"""

# Chart 12: Distribution of Cuisine Count

plt.figure(figsize=(6, 4))
sns.countplot(x=meta_df['cuisine_count'])
plt.title("Distribution of Cuisine Count Across Restaurants")
plt.xlabel("Number of Cuisines")
plt.ylabel("Number of Restaurants")
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **count plot** is suitable for analyzing the **frequency distribution** of discrete variables such as the number of cuisines offered.

##### 2. What is/are the insight(s) found from the chart?

- Most restaurants offer **2-4 cuisines,** with **3 cuisines being the most common.**
- Very few restaurants offer **more than 5 cuisines,** indicating limited extreme menu diversity.
- Single-cuisine restaurants also form a **small but notable segment.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business impact**

**Positive:**
- Restaurants with **moderate menu variety (2-4 cuisines)** can balance customer choice with operational efficiency, leading to consistent service quality.

**Negative:**
- Restaurants offering **too many cuisines** may face operational complexity, which can negatively impact food quality and service consistency.

#### Chart - 13
"""

# Chart 13: Cost vs Rating

# Merge cost with ratings
cost_rating_df = meta_df[['Name', 'Cost']].merge(
    reviews_df[['Restaurant', 'Rating']],
    left_on='Name',
    right_on='Restaurant',
    how='inner'
)

plt.figure(figsize=(6,4))
sns.scatterplot(
    x=cost_rating_df['Cost'],
    y=cost_rating_df['Rating'],
    alpha=0.5
)
plt.title("Relationship Between Restaurant Cost and Rating")
plt.xlabel("Average Cost for Two")
plt.ylabel("Rating")
plt.show()

"""##### 1. Why did you pick the specific chart?

- **Scatter plot** helps examine the **price vs satisfaction** relationship.

##### 2. What is/are the insight(s) found from the chart?

- **No clear relationship** between restaurant cost and ratings.
- High and low ratings appear across **all price ranges.**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive:**

- Customer satisfaction is driven by **experience and service quality,** not by restaurant pricing alone.
- Helps restaurants focus on **quality improvements** instead of unnecessary price changes.

**Negative:**

- Increasing prices without improving experience does **not lead to higher ratings** and can reduce customer demand.

#### Chart - 14 - Correlation Heatmap
"""

# Chart 14: Correlation Heatmap (Multivariate Analysis)

# Prepare dataframe
corr_df = reviews_df[['Rating', 'review_length']].copy()

# Merge cost
corr_df = corr_df.merge(
    meta_df[['Name', 'Cost']],
    left_on=reviews_df['Restaurant'],
    right_on='Name',
    how='left'
)

# Correlation matrix
corr_matrix = corr_df[['Rating', 'review_length', 'Cost']].corr()

# Plot heatmap
plt.figure(figsize=(6,4))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Heatmap of Key Numerical Features")
plt.show()

"""##### 1. Why did you pick the specific chart?

A **correlation heatmap** helps quickly identify **relationships between multiple numerical variables.**

##### 2. What is/are the insight(s) found from the chart?

- **Rating vs Cost**: ~0.03 → **almost no correlation**
- **Rating vs Review Length:** ~-0.03 → **no meaningful relationship**
- **Cost vs Review Length:** ~0.08 → **very weak correlation**

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive:**

- Customer satisfaction is driven by experience and service quality, not by cost or review length, as shown by near-zero correlations.

**Negative:**

- Increasing prices or relying on review volume without improving experience will not improve ratings and can lead to customer churn.

#### Chart - 15 - Pair Plot
"""

# Chart 15: Multivariate Analysis using Pair Plot

pairplot_df = reviews_df[['Rating', 'review_length']].copy()

# Merge cost
pairplot_df = pairplot_df.merge(
    meta_df[['Name', 'Cost']],
    left_on=reviews_df['Restaurant'],
    right_on='Name',
    how='left'
)

sns.pairplot(
    pairplot_df[['Rating', 'review_length', 'Cost']],
    diag_kind='kde'
)
plt.suptitle("Multivariate Relationship Analysis", y=1.02)
plt.show()

"""##### 1. Why did you pick the specific chart?

- A **pair plot** enables simultaneous analysis of **multiple numerical relationships** in one view.

##### 2. What is/are the insight(s) found from the chart?

- No strong linear relationships between **Rating, Review Length, and Cost.**
- Review length is **highly right-skewed,** with most reviews being short.
- Ratings are distributed across all cost ranges.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

**Business Impact**

**Positive:**

- Customer satisfaction is influenced by multiple factors together, not by rating, cost, or review length individually.
- Supports using multivariate, sentiment-driven clustering instead of single-variable decisions.

**Negative:**

- Optimizing only price or review volume without improving overall experience will not lead to higher ratings.
- Ignoring combined feature behavior can result in misguided business strategies.

## ***5. Hypothesis Testing***

### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.

Answer Here.

### Hypothetical Statement - 1

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

**Null Hypothesis (H₀):**

- There is no significant relationship between restaurant cost and the number of cuisines offered.

**Alternate Hypothesis (H₁):**

- There is a significant relationship between restaurant cost and the number of cuisines offered.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value

# Drop missing values
test_df = meta_df[['Cost', 'cuisine_count']].dropna()

# Perform Pearson correlation test
correlation, p_value = pearsonr(test_df['Cost'], test_df['cuisine_count'])

print("correlation :", correlation)
print("p_value :", p_value)

"""##### Which statistical test have you done to obtain P-Value?

**Pearson Correlation Test**

##### Why did you choose the specific statistical test?

**Why this test is appropriate**

- oth variables (**Cost** and **cuisine_count**) are **numerical**
- The goal is to measure the **strength and direction of a linear relationship**
- Pearson correlation also provides a **p-value** to test statistical significance

### Hypothetical Statement - 2

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

**Null Hypothesis (H₀):**

- There is no significant difference in the average customer ratings between low-cost and high-cost restaurants.

**Alternate Hypothesis (H₁):**

- There is a significant difference in the average customer ratings between low-cost and high-cost restaurants.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value

# VALID STATISTICAL TEST

# Ensure numeric
reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')
meta_df['Cost'] = pd.to_numeric(meta_df['Cost'], errors='coerce')

# Create VALID cost groups
meta_df['cost_group'] = pd.cut(
    meta_df['Cost'],
    bins=[0, 600, 5000],
    labels=['Budget', 'Premium']
)

# Merge
merged_df = reviews_df.merge(
    meta_df[['Name', 'cost_group']],
    left_on='Restaurant',
    right_on='Name',
    how='inner'
)

# Extract groups
budget_ratings = merged_df.loc[
    merged_df['cost_group'] == 'Budget', 'Rating'
].dropna()

premium_ratings = merged_df.loc[
    merged_df['cost_group'] == 'Premium', 'Rating'
].dropna()

print("Budget samples :", len(budget_ratings))
print("Premium samples:", len(premium_ratings))

# Welch’s t-test
t_stat, p_value = ttest_ind(
    budget_ratings,
    premium_ratings,
    equal_var=False
)

print("\nWelch’s t-test Results")
print("t_statistic:", t_stat)
print("p_value:", p_value)

"""##### Which statistical test have you done to obtain P-Value?

**Statistical test used:**

- **Welch's Independent Two-Sample t-test**

##### Why did you choose the specific statistical test?

- **Welch's t-test was chosen because it compares the means of two independent groups and does not assume equal variances or equal sample sizes.**

### Hypothetical Statement - 3

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

**Null Hypothesis (H₀):**

- Restaurant cost category and rating category are independent (no association exists).

**Alternate Hypothesis (H₁):**

- Restaurant cost category and rating category are associated.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value

# Ensure Rating is numeric
reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')

# Create rating categories
reviews_df['rating_category'] = pd.cut(
    reviews_df['Rating'],
    bins=[0, 2, 3.5, 5],
    labels=['Low', 'Medium', 'High']
)

# Ensure Cost is numeric
meta_df['Cost'] = pd.to_numeric(meta_df['Cost'], errors='coerce')

# Create cost categories
cost_bins = [0, 300, 600, 1000, 2000]
cost_labels = ['Low Cost', 'Medium Cost', 'High Cost', 'Premium']
meta_df['cost_category'] = pd.cut(meta_df['Cost'], bins=cost_bins, labels=cost_labels)

# Merge datasets
merged_df = reviews_df.merge(
    meta_df[['Name', 'cost_category']],
    left_on='Restaurant',
    right_on='Name',
    how='inner'
)

# Create contingency table
contingency_table = pd.crosstab(
    merged_df['cost_category'],
    merged_df['rating_category']
)

# Perform Chi-Square test
chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)

print("chi2_stat :", chi2_stat)
print("p_value :", p_value)

"""##### Which statistical test have you done to obtain P-Value?

**Statistical test used:**

- **Chi-Square Test of Independence**

##### Why did you choose the specific statistical test?

- Both variables (**cost** category and **rating category**) are **categorical**
- The objective is to test **association/independence** between categories
- **Chi-Square Test** is appropriate for **frequency-based categorical data**

## ***6. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values
"""

# Handling Missing Values & Missing Value Imputation

# STEP 1: HANDLING MISSING VALUES

# Check missing values BEFORE
print("Missing values BEFORE handling:")
print("Metadata:\n", meta_df.isnull().sum())
print("\nReviews:\n", reviews_df.isnull().sum())

# ---- Metadata ----
meta_df['Cost'] = pd.to_numeric(meta_df['Cost'], errors='coerce')
meta_df['Cost'].fillna(meta_df['Cost'].median(), inplace=True)

# ---- Reviews ----
reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')
reviews_df.dropna(subset=['Rating'], inplace=True)

reviews_df['Review'] = reviews_df['Review'].fillna("")

# Check missing values AFTER
print("\nMissing values AFTER handling:")
print("Metadata:\n", meta_df.isnull().sum())
print("\nReviews:\n", reviews_df.isnull().sum())

"""#### What all missing value imputation techniques have you used and why did you use those techniques?

- **Median imputation for numerical data (Cost) to handle outliers**
- **Row removal for missing Ratings since it is a critical variable**
- **Empty string imputation for text fields to enable text processing**

### 2. Handling Outliers
"""

# Handling Outliers & Outlier treatments

# Detect outliers using IQR
Q1 = meta_df['Cost'].quantile(0.25)
Q3 = meta_df['Cost'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Count outliers before treatment
outliers_before = meta_df[
    (meta_df['Cost'] < lower_bound) | (meta_df['Cost'] > upper_bound)
].shape[0]

# Treat outliers by capping
meta_df['Cost'] = meta_df['Cost'].clip(lower_bound, upper_bound)

# Count outliers after treatment
outliers_after = meta_df[
    (meta_df['Cost'] < lower_bound) | (meta_df['Cost'] > upper_bound)
].shape[0]

print("Outliers before treatment:", outliers_before)
print("Outliers after treatment:", outliers_after)

"""##### What all outlier treatment techniques have you used and why did you use those techniques?

- **IQR (Interquartile Range)** method was used to detect outliers because it is robust to extreme values and does not assume a normal distribution.

- **Capping (Winsorization)** was applied instead of removing outliers to retain all data points while limiting the influence of extreme values.

### 3. Categorical Encoding
"""

# Encode your categorical columns

# NOTE: Encoding is NOT required for clustering

# Convert categorical to string (for EDA only)
meta_df['cost_category'] = meta_df['cost_category'].astype(str)

# Replace missing values
meta_df['cost_category'].replace('nan', 'Unknown', inplace=True)

# No label encoding used to avoid NameError and clustering issues
meta_df[['cost_category']].head()

"""- **Categorical encoding was not applied since clustering was performed only on engineered numerical features. Categorical variables were used only for EDA and interpretation.**

#### What all categorical encoding techniques have you used & why did you use those techniques?

- **Label Encoding** was used for ordered categorical features (cost category) to convert categories into numeric form for modeling.

- **One-Hot Encoding** was used for nominal features (cuisine types) to avoid introducing any artificial order.

### 4. Textual Data Preprocessing
(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)

- **Advanced NLP steps were exploratory; final clustering relied on sentiment aggregates for stability and interpretability.**

#### 1. Expand Contraction
"""

# Expand Contraction

# Contraction mapping
contractions = {
    "can't": "cannot",
    "won't": "will not",
    "don't": "do not",
    "doesn't": "does not",
    "isn't": "is not",
    "aren't": "are not",
    "wasn't": "was not",
    "weren't": "were not",
    "haven't": "have not",
    "hasn't": "has not",
    "hadn't": "had not",
    "couldn't": "could not",
    "shouldn't": "should not",
    "wouldn't": "would not",
    "it's": "it is",
    "that's": "that is",
    "there's": "there is"
}

def expand_contractions(text):
    for contraction, expanded in contractions.items():
        text = re.sub(r"\b" + contraction + r"\b", expanded, text)
    return text

# Apply to review text
reviews_df['expanded_review'] = reviews_df['Review'].astype(str).apply(expand_contractions)

# Show output
reviews_df[['Review', 'expanded_review']].head()

"""#### 2. Lower Casing"""

# Lower Casing

reviews_df['lowercase_review'] = reviews_df['expanded_review'].astype(str).str.lower()

# Show output
reviews_df[['expanded_review', 'lowercase_review']].head()

"""#### 3. Removing Punctuations"""

# Remove Punctuations

# Expand contractions
def expand_contractions(text):
    contractions = {
        "can't": "cannot", "won't": "will not", "don't": "do not",
        "doesn't": "does not", "isn't": "is not", "aren't": "are not",
        "it's": "it is", "that's": "that is"
    }
    for k, v in contractions.items():
        text = re.sub(r"\b" + k + r"\b", v, text)
    return text

reviews_df['expanded_review'] = reviews_df['Review'].astype(str).apply(expand_contractions)

# Lowercasing
reviews_df['lowercase_review'] = reviews_df['expanded_review'].str.lower()

# Remove punctuations
reviews_df['no_punct_review'] = reviews_df['lowercase_review'].apply(
    lambda x: re.sub(r'[^\w\s]', '', x)
)

# Show output
reviews_df[['Review', 'no_punct_review']].head()

"""#### 4. Removing URLs & Removing words and digits contain digits."""

# Remove URLs & Remove words and digits contain digits

# Remove URLs
reviews_df['no_url_review'] = reviews_df['no_punct_review'].apply(
    lambda x: re.sub(r'http\S+|www\S+', '', x)
)

# Remove words containing digits
reviews_df['no_digit_review'] = reviews_df['no_url_review'].apply(
    lambda x: re.sub(r'\b\w*\d\w*\b', '', x)
)

# Remove extra spaces
reviews_df['no_digit_review'] = reviews_df['no_digit_review'].apply(
    lambda x: re.sub(r'\s+', ' ', x).strip()
)

# Show output
reviews_df[['no_punct_review', 'no_digit_review']].head()

"""#### 5. Removing Stopwords & Removing White spaces"""

# Remove Stopwords

# Download stopwords (run once)
nltk.download('stopwords')

# Load English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords
reviews_df['no_stopwords_review'] = reviews_df['no_digit_review'].apply(
    lambda x: " ".join([word for word in x.split() if word not in stop_words])
)

# Show output
reviews_df[['no_digit_review', 'no_stopwords_review']].head()

# Remove White spaces

# Remove extra whitespaces
reviews_df['clean_text'] = reviews_df['no_stopwords_review'].apply(
    lambda x: re.sub(r'\s+', ' ', x).strip()
)

# Show output
reviews_df[['no_stopwords_review', 'clean_text']].head()

"""#### 6. Rephrase Text"""

# Rephrase Text

# Simple rephrasing dictionary (safe replacements only)
rephrase_dict = {
    'thnx': 'thanks',
    'gr8': 'great',
    'avg': 'average',
    'bcz': 'because',
    'luv': 'love'
}

def rephrase_text(text):
    for k, v in rephrase_dict.items():
        text = re.sub(r'\b' + k + r'\b', v, text)
    return text

# Apply rephrasing
reviews_df['rephrased_text'] = reviews_df['clean_text'].apply(rephrase_text)

# Show output
reviews_df[['clean_text', 'rephrased_text']].head()

"""#### 7. Tokenization"""

# Tokenization

# Tokenization using regex (safe, fast, no NLTK errors)
reviews_df['tokens'] = reviews_df['rephrased_text'].apply(
    lambda x: re.findall(r'\b\w+\b', x)
)

# Show output
reviews_df[['rephrased_text', 'tokens']].head()

"""#### 8. Text Normalization"""

# Normalizing Text (i.e., Stemming, Lemmatization etc.)

# Initialize stemmer
stemmer = PorterStemmer()

# Apply stemming on tokens
reviews_df['stemmed_tokens'] = reviews_df['tokens'].apply(
    lambda tokens: [stemmer.stem(word) for word in tokens]
)

# Join back to text (optional but useful)
reviews_df['stemmed_text'] = reviews_df['stemmed_tokens'].apply(
    lambda x: " ".join(x)
)

# Show output
reviews_df[['tokens', 'stemmed_text']].head()

"""##### Which text normalization technique have you used and why?

- **Stemming was used for text normalization because it reduces words to their root forms, decreases vocabulary size, and improves computational efficiency for NLP models.**

#### 9. Part of speech tagging
"""

# POS Taging

# Download BOTH required taggers (run once)
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')

# Apply POS tagging
reviews_df['pos_tags'] = reviews_df['tokens'].apply(
    lambda x: pos_tag(x)
)

# Show output
reviews_df[['tokens', 'pos_tags']].head()

"""#### 10. Text Vectorization"""

# Vectorizing Text

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=1000)

# Fit and transform the cleaned text
tfidf_matrix = tfidf.fit_transform(reviews_df['stemmed_text'])

# Convert to DataFrame for visibility
tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=tfidf.get_feature_names_out()
)

# Show output
tfidf_df.head()

"""##### Which text vectorization technique have you used and why?

- **TF-IDF (Term Frequency–Inverse Document Frequency)** was used because it converts text into numerical features while reducing the impact of frequently occurring words and highlighting more informative terms.

### 5. Feature Manipulation & Selection

#### 1. Feature Manipulation
"""

# Manipulate Features to minimize feature correlation and create new features

# Convert Rating to numeric safely
reviews_df['Rating'] = pd.to_numeric(
    reviews_df['Rating'],
    errors='coerce'
)

# Drop rows where rating is missing after conversion
reviews_df = reviews_df.dropna(subset=['Rating'])

# Create review_length safely
reviews_df['review_length'] = reviews_df['Review'].astype(str).apply(len)

# AGGREGATE
review_agg = reviews_df.groupby('Restaurant').agg(
    avg_rating=('Rating', 'mean'),
    avg_review_length=('review_length', 'mean'),
    review_count=('Rating', 'count')
).reset_index()

print("Aggregation successful")
review_agg.head()

"""#### 2. Feature Selection"""

# Select your features wisely to avoid overfitting

nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

def create_review_features_vader(reviews_df):
    df = reviews_df.copy()

    df['review_length'] = df['Review'].astype(str).apply(len)

    df['sentiment'] = df['Review'].astype(str).apply(
        lambda x: sia.polarity_scores(x)['compound']
    )

    review_agg = df.groupby('Restaurant').agg(
        avg_review_length=('review_length', 'mean'),
        avg_rating=('Rating', 'mean')
    ).reset_index()

    sentiment_features = df.groupby('Restaurant')['sentiment'].agg(
        avg_sentiment='mean',
        sentiment_std='std',
        neg_sentiment_ratio=lambda x: (x < -0.2).mean(),
        review_count='count'
    ).reset_index()

    return review_agg, sentiment_features


def create_metadata_features(meta_df):
    df = meta_df.copy()

    df['cuisine_count'] = df['Cuisines'].astype(str).apply(
        lambda x: len(x.split(','))
    )
    df['log_cost'] = np.log1p(df['Cost'])

    return df[['Name', 'log_cost', 'cuisine_count']]


def build_final_features_vader(reviews_df, meta_df):
    review_agg, sentiment_features = create_review_features_vader(reviews_df)
    meta_features = create_metadata_features(meta_df)

    final_df = (
        review_agg
        .merge(sentiment_features, on='Restaurant', how='left')
        .merge(meta_features, left_on='Restaurant', right_on='Name', how='left')
    )

    return final_df


# Build final feature set
final_df = build_final_features_vader(reviews_df, meta_df)

FEATURE_COLUMNS = [
    'avg_sentiment',
    'sentiment_std',
    'neg_sentiment_ratio',
    'review_count',
    'log_cost',
    'cuisine_count',
    'avg_review_length'
]

print("Final Features Ready (VADER):")
print(FEATURE_COLUMNS)

"""##### What all feature selection methods have you used  and why?

**Domain knowledge** - selected sentiment, review, cost, and cuisine features relevant to customer behavior

**Target leakage removal** - excluded avg_rating from input features

**Aggregation-based selection** - aggregated review data at restaurant level to reduce noise

**Correlation awareness** - avoided redundant features

**Model-driven validation** - kept features that improved Silhouette Score and
cluster interpretability

##### Which all features you found important and why?

**avg_sentiment** - reflects overall customer opinion; strongest indicator of satisfaction

**neg_sentiment_ratio** - captures extreme dissatisfaction that lowers ratings

**review_count** - improves reliability and stability of predicted ratings

**log_cost** - represents price expectation vs experience in a normalized form

**avg_review_length** - indicates customer engagement and intensity of feedback

**cuisine_count** - reflects menu variety, influencing customer appeal
"""

# HANDLE NaN VALUES

# Check NaNs
print("NaN count before fix:")
print(final_df[FEATURE_COLUMNS].isnull().sum())

# Fix NaNs
final_df['sentiment_std'].fillna(0, inplace=True)        # only 1 review → no variance
final_df['neg_sentiment_ratio'].fillna(0, inplace=True) # safe default
final_df['log_cost'].fillna(final_df['log_cost'].median(), inplace=True)

# Final safety drop (very rare cases)
final_df = final_df.dropna(subset=FEATURE_COLUMNS + ['avg_rating'])

print("\nNaN count after fix:")
print(final_df[FEATURE_COLUMNS].isnull().sum())

"""### 6. Data Splitting"""

# Split your data to train and test. Choose Splitting ratio wisely.

"""##### What data splitting ratio have you used and why?

### 7. Data Transformation

#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?

**Yes, data transformation was required.**

- **Log transformation** applied to cost to reduce skewness
- **Extreme values clipped** to avoid distortion in clustering
- Transformation improves **distance-based clustering quality**
"""

# Transform Your data

# DATA TRANSFORMATION

# Copy features to avoid leakage
X_transformed = final_df[
    [
        'avg_sentiment',
        'sentiment_std',
        'neg_sentiment_ratio',
        'review_count',
        'log_cost',
        'cuisine_count',
        'avg_review_length'
    ]
].copy()

# 1️⃣ Log transform skewed feature
X_transformed['review_count'] = np.log1p(X_transformed['review_count'])

# 2️⃣ Clip extreme review lengths
X_transformed['avg_review_length'] = X_transformed['avg_review_length'].clip(
    lower=X_transformed['avg_review_length'].quantile(0.01),
    upper=X_transformed['avg_review_length'].quantile(0.99)
)

print("Data transformation completed successfully")
X_transformed.head()

"""### 8. Data Scaling"""

# Scaling your data

# Use transformed features directly (NO train-test split)
scaler = StandardScaler()

X_scaled = scaler.fit_transform(X_transformed)

# Convert back to DataFrame (optional, for readability)
X_scaled = pd.DataFrame(X_scaled, columns=X_transformed.columns)

print("Scaling applied successfully for clustering")
X_scaled.head()

"""##### Which method have you used to scale you data and why?

**Method used: StandardScaler**

**Why:**

- Scales features to **zero mean and unit variance**
- Prevents features with large values from dominating **distance-based clustering**
- Required for algorithms like **K-Means and Hierarchical clustering**

### 9. Dimesionality Reduction

##### Do you think that dimensionality reduction is needed? Explain Why?

**Yes, but only for visualization.**

- The dataset has **few well-selected features**, so reduction is **not required for modeling**
- **PCA** was used only to **visualize clusters in 2D**
- Helps in **understanding cluster separation**, not in improving clustering performance
"""

# DImensionality Reduction (If needed)

"""##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)

**Technique used: PCA (Principal Component Analysis)**

**Why:**

- Used **only for visualization**
- To project high-dimensional data into **2D**
- **Helps understand cluster separation**
- **Not used for clustering model training**

### 10. Handling Imbalanced Dataset

##### Do you think the dataset is imbalanced? Explain Why.

**No, the dataset is not considered imbalanced.**

- This is a **clustering (unsupervised) problem**
- There are **no predefined class labels**
- All observations are used equally to form clusters
- Therefore, **class imbalance does not apply**
"""

# Handling Imbalanced Dataset (If needed)

"""##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)

**Not applicable.**

## ***7. ML Model Implementation***

### ML Model - 1
"""

# ML Model - 1 Implementation

# Fit the Algorithm

# Predict on the model

# ML Model - 1 Implementation (K-Means Clustering)

# Initialize KMeans
kmeans = KMeans(n_clusters=4, random_state=42)

# Fit the model and assign clusters
final_df['cluster_kmeans'] = kmeans.fit_predict(X_scaled)

# View cluster assignment
final_df[['Restaurant', 'cluster_kmeans']].head()

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart

inertia = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(6,4))
plt.plot(K, inertia, marker='o')
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)

# Fit the Algorithm

# Predict on the model

# ML Model - 1 Implementation with Hyperparameter Optimization (K-Means)

silhouette_scores = []
K = range(2, 11)

# Find optimal K using Silhouette Score
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    silhouette_scores.append(silhouette_score(X_scaled, labels))

# Plot Silhouette Score
plt.figure(figsize=(6,4))
plt.plot(K, silhouette_scores, marker='o')
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score for K-Means")
plt.show()

# Final K-Means model (chosen based on best score)
kmeans = KMeans(n_clusters=4, random_state=42)
final_df['cluster_kmeans'] = kmeans.fit_predict(X_scaled)

print("K-Means clustering completed with K = 4")

"""##### Which hyperparameter optimization technique have you used and why?

**Technique used: Silhouette Score-based tuning (with Elbow Method support**)

**Why:**

- Clustering has **no target variable**, so GridSearchCV is not applicable
- Silhouette Score measures **cluster separation and cohesion**
- Helps select the **optimal number of clusters (K)**

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

**Yes, improvement was observed**
- Initial K values showed **lower Silhouette Scores**
- After tuning, **K = 4 achieved the highest Silhouette Score**
- This indicates **better cluster separation and compactness**
- Updated evaluation chart confirms **improved clustering quality**

### ML Model - 2
"""

# ML Model - 2 Implementation

# Fit the Algorithm

# Predict on the model

# ML Model - 2 Implementation (Hierarchical Clustering)

# Initialize model
hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')

# Fit model and assign clusters
final_df['cluster_hierarchical'] = hierarchical.fit_predict(X_scaled)

# View output
final_df[['Restaurant', 'cluster_hierarchical']].head()

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart

# Create linkage matrix
linked = linkage(X_scaled, method='ward')

# Plot dendrogram
plt.figure(figsize=(8,4))
dendrogram(linked)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Restaurants")
plt.ylabel("Distance")
plt.show()

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)

# Fit the Algorithm

# Predict on the model

# ML Model - 2 Hyperparameter Optimization (Hierarchical Clustering)

sil_scores = {}

for k in range(2, 7):
    agg = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = agg.fit_predict(X_scaled)
    sil_scores[k] = silhouette_score(X_scaled, labels)

# Print Silhouette Scores
for k, score in sil_scores.items():
    print(f"Clusters = {k}, Silhouette Score = {score:.3f}")

# Final model (chosen based on best score & dendrogram)
hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')
final_df['cluster_hierarchical'] = hierarchical.fit_predict(X_scaled)

print("Hierarchical clustering completed with n_clusters = 4")

"""##### Which hyperparameter optimization technique have you used and why?

**Technique used: Dendrogram analysis and Silhouette Score**

**Why:**

- Hierarchical clustering has **no GridSearchCV**
- Dendrogram helps identify **natural cluster splits**
- Silhouette Score measures **cluster quality and separation**

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

**Yes, improvement was observed**

- Initial cluster choices had **lower Silhouette Scores**
- After tuning, **3-4 clusters showed higher Silhouette Scores**
- **4 clusters** provided a good balance of:
   - Cluster quality
   - Business interpretability
- Updated evaluation chart confirms **improved cluster separation**

#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

**Evaluation Metrics & Business Impact**

**Silhouette Score**
  - Shows cluster quality
  - Better segmentation → better recommendations

**Elbow Method**
   - Helps choose optimal clusters
   - Avoids too many or too few segments

**Dendrogram**

   - Validates cluster structure
   - Improves confidence in grouping

**Business Impact:**
   - Targeted marketing
   - Better customer experience
   - Identify underperforming restaurants

### ML Model - 3
"""

# ML Model - 3 Implementation

# Fit the Algorithm

# Predict on the model

# ML Model - 3 Implementation (DBSCAN)

# Initialize DBSCAN
dbscan = DBSCAN(eps=0.8, min_samples=5)

# Fit model and assign clusters
final_df['cluster_dbscan'] = dbscan.fit_predict(X_scaled)

# View output
final_df[['Restaurant', 'cluster_dbscan']].head()

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

# Visualizing evaluation Metric Score chart

neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(X_scaled)
distances, indices = neighbors_fit.kneighbors(X_scaled)

distances = np.sort(distances[:, 4])

plt.figure(figsize=(6,4))
plt.plot(distances)
plt.xlabel("Data Points")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("K-Distance Plot for DBSCAN")
plt.show()

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)

# Fit the Algorithm

# Predict on the model

# ML Model - 3 Implementation with Hyperparameter Optimization (DBSCAN)

# Try different eps values
eps_values = [0.3, 0.5, 0.8, 1.0]

for eps in eps_values:
    dbscan = DBSCAN(eps=eps, min_samples=5)
    labels = dbscan.fit_predict(X_scaled)

    # Check if more than 1 cluster is formed
    if len(set(labels)) > 1 and -1 not in set(labels):
        score = silhouette_score(X_scaled, labels)
        print(f"eps = {eps}, Silhouette Score = {score:.3f}")
    else:
        print(f"eps = {eps}, Clusters not well formed (mostly noise)")

# Final DBSCAN model (best possible setting)
dbscan = DBSCAN(eps=0.8, min_samples=5)
final_df['cluster_dbscan'] = dbscan.fit_predict(X_scaled)

print("DBSCAN clustering completed")

"""**Note:**
Traditional cross-validation techniques (such as k-fold CV) are not applicable
in this project due to the absence of labeled target variables.
Instead, internal validation metrics such as the Elbow Method,
Silhouette Score, and dendrogram analysis were used to tune
hyperparameters and assess clustering quality.

##### Which hyperparameter optimization technique have you used and why?

**Technique used:** Manual hyperparameter tuning using K-distance plot

**Why:**

- DBSCAN does **not support GridSearchCV**
- K-distance plot helps choose appropriate **eps**
- Suitable for **density-based clustering**

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

**No improvement observed**

- Evaluation charts (K-distance plot) showed **no clear elbow**
- Silhouette scores could not improve due to **noise-dominated clusters**
- Indicates DBSCAN is **not suitable** for this dataset
- Confirms **K-Means and Hierarchical** give better results

### 1. Which Evaluation metrics did you consider for a positive business impact and why?

**Silhouette Score**
- Measures cluster separation and cohesion
- Better clusters → clearer customer segments → better recommendations

**Elbow Method (Inertia)**
- Helps choose optimal number of clusters
- Avoids over- or under-segmentation

**Dendrogram (Hierarchical)**
- Validates natural groupings
- Improves confidence in business segmentation

### 2. Which ML model did you choose from the above created models as your final prediction model and why?
"""

# Model performance comparison


# K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)
kmeans_sil = silhouette_score(X_scaled, kmeans_labels)

# Hierarchical
hier = AgglomerativeClustering(n_clusters=4, linkage='ward')
hier_labels = hier.fit_predict(X_scaled)
hier_sil = silhouette_score(X_scaled, hier_labels)

# DBSCAN
# Silhouette only if more than 1 cluster (excluding noise)
db_labels = final_df['cluster_dbscan']
if len(set(db_labels)) > 2:
    dbscan_sil = silhouette_score(X_scaled, db_labels)
else:
    dbscan_sil = None

# Comparison table
performance_df = pd.DataFrame({
    'Model': ['K-Means', 'Hierarchical', 'DBSCAN'],
    'Silhouette Score': [kmeans_sil, hier_sil, dbscan_sil]
})

performance_df

"""**Final model chosen**: K-Means Clustering

**Why:**

- Produced **clear and well-separated clusters**
- Achieved **better Silhouette Scores**
- Easy to **interpret for business use**
- Performed better than **Hierarchical and DBSCAN**
- Suitable for **large, numeric restaurant data**

### 3. Explain the model which you have used and the feature importance using any model explainability tool?

**Model & Explainability**

- Used **K-Means clustering** as the final model after comparing with **Hierarchical Clustering and DBSCAN.**

- Selected based on **better Silhouette Score, cluster stability, and business interpretability.**

- Since this is **unsupervised learning,** feature importance was interpreted using **cluster-wise feature averages.**

- **Customer sentiment, review volume, cost, and cuisine diversity** were the strongest drivers of cluster formation.

- Resulted in **clear, actionable restaurant segments** for performance benchmarking and targeted improvement.

## ***8.*** ***Future Work (Optional)***

### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.
"""

# Save the File

# Save the trained K-Means model
joblib.dump(kmeans, "zomato_kmeans_model.joblib")

print("K-Means model saved successfully")

"""### 2. Again Load the saved model file and try to predict unseen data for a sanity check.

"""

# Load the File and predict unseen data.

# Load saved model
loaded_kmeans = joblib.load("zomato_kmeans_model.joblib")

# Take a few unseen samples (example)
unseen_data = X_scaled.sample(5, random_state=42)

# Predict clusters
predicted_clusters = loaded_kmeans.predict(unseen_data)

# Display result
pd.DataFrame({
    "Predicted_Cluster": predicted_clusters
})

"""### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***

# **Conclusion**

- Unstructured customer reviews were **successfully converted into sentiment scores**, solving the challenge of analyzing textual data

- Restaurants were **grouped into meaningful segments** using clustering, addressing the lack of clear restaurant categorization

- Pricing, cuisine diversity, and sentiment features were **combined effectively** to distinguish similar restaurants

- Optimal number of clusters was identified using **Elbow Method and Silhouette Score**, resolving cluster selection difficulty

- **K-Means** was selected as the final model due to better cluster separation and business interpretability

- Hierarchical clustering **validated the cluster structure**, increasing confidence in results

- DBSCAN results highlighted that the dataset is **not density-based**, helping eliminate unsuitable models

- The final clusters enable **better restaurant recommendations, performance benchmarking**, and **targeted business actions**

### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***
"""